{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd6e06e-2b47-40e5-b645-0ee90b987ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake News Detection – Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af7cc83-1f88-4f9e-896a-8da23ef75de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010c4a7f-689b-4d3e-9c78-db0d56fe47a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10239, 15)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m test_df = load_data(processed_dir + \u001b[33m\"\u001b[39m\u001b[33mtest_clean.tsv\u001b[39m\u001b[33m\"\u001b[39m, raw_dir + \u001b[33m\"\u001b[39m\u001b[33mtest.tsv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain shape:\u001b[39m\u001b[33m\"\u001b[39m, train_df.shape)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLabels:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique())\n\u001b[32m     34\u001b[39m train_df.head(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to load data safely\n",
    "def load_data(file_path, fallback_path):\n",
    "    try:\n",
    "        if Path(file_path).exists():\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    except:\n",
    "        df = pd.read_csv(fallback_path, sep=\"\\t\")\n",
    "        if \"clean_statement\" not in df.columns:\n",
    "            df[\"clean_statement\"] = df.get(\"statement\", \"\")  # Using get() for safer access\n",
    "    \n",
    "    # Ensure clean_statement exists and isn't empty\n",
    "    if \"clean_statement\" not in df.columns:\n",
    "        df[\"clean_statement\"] = df.get(\"statement\", \"\")\n",
    "    df[\"clean_statement\"] = df[\"clean_statement\"].fillna(\"\")  # Fill any NaN values\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load datasets\n",
    "data_dir = \"../data/\"\n",
    "processed_dir = data_dir + \"processed/\"\n",
    "raw_dir = data_dir + \"raw/\"\n",
    "\n",
    "train_df = load_data(processed_dir + \"train_clean.tsv\", raw_dir + \"train.tsv\")\n",
    "valid_df = load_data(processed_dir + \"valid_clean.tsv\", raw_dir + \"valid.tsv\")\n",
    "test_df = load_data(processed_dir + \"test_clean.tsv\", raw_dir + \"test.tsv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Labels:\", train_df['label'].unique())\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6931a2f-20ac-4550-a747-6ea4d9af0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Variables\n",
    "X_train = train_df['clean_statement']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_valid = valid_df['clean_statement']\n",
    "y_valid = valid_df['label']\n",
    "\n",
    "X_test  = test_df['clean_statement']\n",
    "y_test  = test_df['label']\n",
    "\n",
    "print(\"Training Samples:\", len(X_train))\n",
    "print(\"Validation Samples:\", len(X_valid))\n",
    "print(\"Test Samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3ad78-6195-4507-a6de-894eeb231657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf.transform(X_valid)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shape (Train):\", X_train_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712d85f-c370-4147-af42-1d741cf4b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Logistic Regression \n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "log_reg_cv = GridSearchCV(log_reg, param_grid, cv=5, scoring=\"f1_weighted\", n_jobs=-1)\n",
    "log_reg_cv.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best Logistic Regression Parameters:\", log_reg_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240bf24a-4b1c-4052-8d8f-0117dccc5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Evaluation\n",
    "y_pred_valid = log_reg_cv.predict(X_valid_tfidf)\n",
    "\n",
    "print(\"Logistic Regression Validation Results:\")\n",
    "print(classification_report(y_valid, y_pred_valid))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_valid, labels=log_reg_cv.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=log_reg_cv.classes_, yticklabels=log_reg_cv.classes_)\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.savefig(\"../results/confusion_logistic.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad79e19-db78-426e-96f0-92363323d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Evaluation\n",
    "y_pred_valid = log_reg_cv.predict(X_valid_tfidf)\n",
    "\n",
    "print(\"Logistic Regression Validation Results:\")\n",
    "print(classification_report(y_valid, y_pred_valid))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_valid, labels=log_reg_cv.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=log_reg_cv.classes_, yticklabels=log_reg_cv.classes_)\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.savefig(\"../results/confusion_logistic.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad25a08-4507-4a83-90c6-1a2b0a7fd8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Evaluation\n",
    "y_pred_valid_rf = rf_cv.predict(X_valid_tfidf)\n",
    "\n",
    "print(\"Random Forest Validation Results:\")\n",
    "print(classification_report(y_valid, y_pred_valid_rf))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_valid_rf, labels=rf_cv.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=rf_cv.classes_, yticklabels=rf_cv.classes_)\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.savefig(\"../results/confusion_rf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a68707-52b7-461a-be12-6958c8597b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Evaluation (\n",
    "print(\"===== Final Evaluation on Test Set =====\")\n",
    "\n",
    "print(\"\\n[Logistic Regression]\")\n",
    "y_pred_test = log_reg_cv.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\n[Random Forest]\")\n",
    "y_pred_test_rf = rf_cv.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_test_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd3fed-2e7d-4f8c-a055-af53eee95475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Models & Vectorize\n",
    "# Save vectorizer and best models\n",
    "pickle.dump(tfidf, open(\"../models/tfidf_vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(log_reg_cv.best_estimator_, open(\"../models/tfidf_logistic.pkl\", \"wb\"))\n",
    "pickle.dump(rf_cv.best_estimator_, open(\"../models/tfidf_rf.pkl\", \"wb\"))\n",
    "\n",
    "print(\"✅ Models and vectorizer saved to /models/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
