{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd6e06e-2b47-40e5-b645-0ee90b987ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake News Detection â€“ Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af7cc83-1f88-4f9e-896a-8da23ef75de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010c4a7f-689b-4d3e-9c78-db0d56fe47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (10239, 15)\n",
      "Warning: 'label' column not found in DataFrame. Available columns: ['2635.json', 'false', 'Says the Annies List political group supports third-trimester abortions on demand.', 'abortion', 'dwayne-bohac', 'State representative', 'Texas', 'republican', '0', '1', '0.1', '0.2', '0.3', 'a mailer', 'clean_statement']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2635.json</th>\n",
       "      <th>false</th>\n",
       "      <th>Says the Annies List political group supports third-trimester abortions on demand.</th>\n",
       "      <th>abortion</th>\n",
       "      <th>dwayne-bohac</th>\n",
       "      <th>State representative</th>\n",
       "      <th>Texas</th>\n",
       "      <th>republican</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>a mailer</th>\n",
       "      <th>clean_statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    2635.json        false  \\\n",
       "0  10540.json    half-true   \n",
       "1    324.json  mostly-true   \n",
       "2   1123.json        false   \n",
       "\n",
       "  Says the Annies List political group supports third-trimester abortions on demand.  \\\n",
       "0  When did the decline of coal start? It started...                                   \n",
       "1  Hillary Clinton agrees with John McCain \"by vo...                                   \n",
       "2  Health care reform legislation is likely to ma...                                   \n",
       "\n",
       "                             abortion    dwayne-bohac State representative  \\\n",
       "0  energy,history,job-accomplishments  scott-surovell       State delegate   \n",
       "1                      foreign-policy    barack-obama            President   \n",
       "2                         health-care    blog-posting                  NaN   \n",
       "\n",
       "      Texas republican     0     1    0.1    0.2   0.3         a mailer  \\\n",
       "0  Virginia   democrat   0.0   0.0    1.0    1.0   0.0  a floor speech.   \n",
       "1  Illinois   democrat  70.0  71.0  160.0  163.0   9.0           Denver   \n",
       "2       NaN       none   7.0  19.0    3.0    5.0  44.0   a news release   \n",
       "\n",
       "  clean_statement  \n",
       "0                  \n",
       "1                  \n",
       "2                  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to load data safely\n",
    "def load_data(file_path, fallback_path):\n",
    "    try:\n",
    "        if Path(file_path).exists():\n",
    "            df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        else:\n",
    "            df = pd.read_csv(fallback_path, sep=\"\\t\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Ensure clean_statement exists and isn't empty\n",
    "    if \"clean_statement\" not in df.columns:\n",
    "        df[\"clean_statement\"] = df.get(\"statement\", \"\")\n",
    "    df[\"clean_statement\"] = df[\"clean_statement\"].fillna(\"\")  # Fill any NA values\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load datasets\n",
    "data_dir = \"../data/\"\n",
    "processed_dir = data_dir + \"processed/\"\n",
    "raw_dir = data_dir + \"raw/\"\n",
    "\n",
    "train_df = load_data(processed_dir + \"train_clean.tsv\", raw_dir + \"train.tsv\")\n",
    "valid_df = load_data(processed_dir + \"valid_clean.tsv\", raw_dir + \"valid.tsv\")\n",
    "test_df = load_data(processed_dir + \"test_clean.tsv\", raw_dir + \"test.tsv\")\n",
    "\n",
    "print(\"train shape:\", train_df.shape)\n",
    "\n",
    "# First check if 'label' column exists before accessing it\n",
    "if 'label' in train_df.columns:\n",
    "    print(\"labels:\", train_df[\"label\"].unique())\n",
    "else:\n",
    "    print(\"Warning: 'label' column not found in DataFrame. Available columns:\", train_df.columns.tolist())\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6931a2f-20ac-4550-a747-6ea4d9af0d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: ['2635.json', 'false', 'Says the Annies List political group supports third-trimester abortions on demand.', 'abortion', 'dwayne-bohac', 'State representative', 'Texas', 'republican', '0', '1', '0.1', '0.2', '0.3', 'a mailer', 'clean_statement']\n",
      "Valid columns: ['12134.json', 'barely-true', 'We have less Americans working now than in the 70s.', 'economy,jobs', 'vicky-hartzler', 'U.S. Representative', 'Missouri', 'republican', '1', '0', '1.1', '0.1', '0.2', 'an interview with ABC17 News', 'clean_statement']\n",
      "Test columns: ['11972.json', 'true', 'Building a wall on the U.S.-Mexico border will take literally years.', 'immigration', 'rick-perry', 'Governor', 'Texas', 'republican', '30', '30.1', '42', '23', '18', 'Radio interview', 'clean_statement']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train columns:\", train_df.columns.tolist())\n",
    "print(\"Valid columns:\", valid_df.columns.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf950d57-3570-4bf6-8a04-acf08da24497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data splits:\n",
      "Training samples: 10239 (Labels: ['half-true', 'mostly-true', 'false', 'true', 'barely-true', 'pants-fire'])\n",
      "Validation samples: 1283 (Labels: ['pants-fire', 'false', 'half-true', 'true', 'barely-true', 'mostly-true'])\n",
      "Test samples: 1266 (Labels: ['false', 'half-true', 'pants-fire', 'true', 'barely-true', 'mostly-true'])\n",
      "\n",
      "Sample training data:\n",
      "  text        label\n",
      "0         half-true\n",
      "1       mostly-true\n",
      "2             false\n"
     ]
    }
   ],
   "source": [
    "# Split Variables using the correct column names\n",
    "# The label appears to be the second column (index 1) in each DataFrame\n",
    "X_train = train_df['clean_statement']\n",
    "y_train = train_df.iloc[:, 1]  # Get second column by position\n",
    "X_valid = valid_df['clean_statement']\n",
    "y_valid = valid_df.iloc[:, 1]\n",
    "X_test = test_df['clean_statement']\n",
    "y_test = test_df.iloc[:, 1]\n",
    "\n",
    "# Verify the splits\n",
    "print(\"\\nData splits:\")\n",
    "print(f\"Training samples: {len(X_train)} (Labels: {y_train.unique().tolist()})\")\n",
    "print(f\"Validation samples: {len(X_valid)} (Labels: {y_valid.unique().tolist()})\")\n",
    "print(f\"Test samples: {len(X_test)} (Labels: {y_test.unique().tolist()})\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample training data:\")\n",
    "print(pd.DataFrame({'text': X_train.head(3), 'label': y_train.head(3)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b3ad78-6195-4507-a6de-894eeb231657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying input data...\n",
      "X_train sample: \n",
      "X_train type: <class 'pandas.core.series.Series'>\n",
      "\n",
      "Error during TF-IDF vectorization: empty vocabulary; perhaps the documents only contain stop words\n",
      "Troubleshooting steps:\n",
      "1. Verify X_train contains text data\n",
      "2. Check for None or NaN values in text\n",
      "3. Ensure sklearn version >= 0.24.0 for get_feature_names_out()\n",
      "Sample data check: 0    \n",
      "Name: clean_statement, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Fixed typo in \"extraction\"\n",
    "\n",
    "# First verify your input data\n",
    "print(\"Verifying input data...\")\n",
    "print(f\"X_train sample: {X_train.iloc[0] if hasattr(X_train, 'iloc') else X_train[0]}\")\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "\n",
    "# TF-IDF Vectorization with error handling\n",
    "try:\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        lowercase=True,\n",
    "        analyzer='word',\n",
    "        min_df=5,\n",
    "        max_df=0.7\n",
    "    )\n",
    "\n",
    "    # Ensure data is in correct format (convert to list if needed)\n",
    "    if hasattr(X_train, 'values'):\n",
    "        X_train_text = X_train.values.astype('U')  # Convert to Unicode\n",
    "    else:\n",
    "        X_train_text = list(X_train)\n",
    "    \n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    X_valid_tfidf = tfidf.transform(X_valid)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    print(\"\\nTF-IDF Vectorization Successful!\")\n",
    "    print(\"Train shape:\", X_train_tfidf.shape)\n",
    "    print(\"Validation shape:\", X_valid_tfidf.shape)\n",
    "    print(\"Test shape:\", X_test_tfidf.shape)\n",
    "    \n",
    "    # Get feature names safely\n",
    "    try:\n",
    "        features = tfidf.get_feature_names_out()\n",
    "        print(f\"\\nNumber of features: {len(features)}\")\n",
    "        print(\"Sample features:\", features[:20])\n",
    "    except AttributeError:\n",
    "        # For older sklearn versions\n",
    "        features = tfidf.get_feature_names()\n",
    "        print(f\"\\nNumber of features: {len(features)}\")\n",
    "        print(\"Sample features:\", features[:20])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during TF-IDF vectorization: {str(e)}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"1. Verify X_train contains text data\")\n",
    "    print(\"2. Check for None or NaN values in text\")\n",
    "    print(\"3. Ensure sklearn version >= 0.24.0 for get_feature_names_out()\")\n",
    "    print(f\"Sample data check: {X_train[:1] if len(X_train) > 0 else 'Empty data!'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9712d85f-c370-4147-af42-1d741cf4b3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying input data...\n",
      "X_train sample: \n",
      "X_train type: <class 'pandas.core.series.Series'>\n",
      "\n",
      "Error during TF-IDF vectorization: empty vocabulary; perhaps the documents only contain stop words\n",
      "Troubleshooting steps:\n",
      "1. Verify X_train contains text data\n",
      "2. Check for None or NaN values in text\n",
      "3. Ensure sklearn version >= 0.24.0 for get_feature_names_out()\n",
      "Sample data check: 0    \n",
      "Name: clean_statement, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Fixed typo in \"extraction\"\n",
    "\n",
    "# First verify your input data\n",
    "print(\"Verifying input data...\")\n",
    "print(f\"X_train sample: {X_train.iloc[0] if hasattr(X_train, 'iloc') else X_train[0]}\")\n",
    "print(f\"X_train type: {type(X_train)}\")\n",
    "\n",
    "# TF-IDF Vectorization with error handling\n",
    "try:\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        lowercase=True,\n",
    "        analyzer='word',\n",
    "        min_df=5,\n",
    "        max_df=0.7\n",
    "    )\n",
    "\n",
    "    # Ensure data is in correct format (convert to list if needed)\n",
    "    if hasattr(X_train, 'values'):\n",
    "        X_train_text = X_train.values.astype('U')  # Convert to Unicode\n",
    "    else:\n",
    "        X_train_text = list(X_train)\n",
    "    \n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    X_valid_tfidf = tfidf.transform(X_valid)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "    print(\"\\nTF-IDF Vectorization Successful!\")\n",
    "    print(\"Train shape:\", X_train_tfidf.shape)\n",
    "    print(\"Validation shape:\", X_valid_tfidf.shape)\n",
    "    print(\"Test shape:\", X_test_tfidf.shape)\n",
    "    \n",
    "    # Get feature names safely\n",
    "    try:\n",
    "        features = tfidf.get_feature_names_out()\n",
    "        print(f\"\\nNumber of features: {len(features)}\")\n",
    "        print(\"Sample features:\", features[:20])\n",
    "    except AttributeError:\n",
    "        # For older sklearn versions\n",
    "        features = tfidf.get_feature_names()\n",
    "        print(f\"\\nNumber of features: {len(features)}\")\n",
    "        print(\"Sample features:\", features[:20])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during TF-IDF vectorization: {str(e)}\")\n",
    "    print(\"Troubleshooting steps:\")\n",
    "    print(\"1. Verify X_train contains text data\")\n",
    "    print(\"2. Check for None or NaN values in text\")\n",
    "    print(\"3. Ensure sklearn version >= 0.24.0 for get_feature_names_out()\")\n",
    "    print(f\"Sample data check: {X_train[:1] if len(X_train) > 0 else 'Empty data!'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240bf24a-4b1c-4052-8d8f-0117dccc5797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "Warning: empty vocabulary; perhaps the documents only contain stop words - Using fallback parameters\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      5\u001b[39m tfidf = TfidfVectorizer(\n\u001b[32m      6\u001b[39m     max_features=\u001b[32m5000\u001b[39m,\n\u001b[32m      7\u001b[39m     ngram_range=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     analyzer=\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m X_train_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure string type\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tfidf.vocabulary_) == \u001b[32m0\u001b[39m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Fallback if vocabulary is empty\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1284\u001b[39m         )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Fallback with minimal filtering\u001b[39;00m\n\u001b[32m     23\u001b[39m     tfidf = TfidfVectorizer(\n\u001b[32m     24\u001b[39m         ngram_range=(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m),  \u001b[38;5;66;03m# Simpler ngrams\u001b[39;00m\n\u001b[32m     25\u001b[39m         stop_words=\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Don't remove stop words\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m         analyzer=\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     30\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     X_train_tfidf = \u001b[43mtfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Transform validation data\u001b[39;00m\n\u001b[32m     34\u001b[39m X_valid_tfidf = tfidf.transform(X_valid.astype(\u001b[38;5;28mstr\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\fake-news-detection\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1280\u001b[39m     vocabulary = \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1284\u001b[39m         )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 3. TF-IDF Vectorization - Robust Version\n",
    "print(\"Creating TF-IDF features...\")\n",
    "\n",
    "# First check if input data exists and is valid\n",
    "if len(X_train) == 0:\n",
    "    raise ValueError(\"Training data is empty - cannot create features\")\n",
    "\n",
    "# Initialize vectorizer with robust parameters\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),          # Use both unigrams and bigrams\n",
    "    stop_words='english',        # Remove common English words\n",
    "    min_df=3,                    # Minimum 3 documents for a term\n",
    "    max_df=0.85,                 # Maximum 85% document frequency\n",
    "    lowercase=True,              # Convert to lowercase\n",
    "    analyzer='word',             # Tokenize by words\n",
    "    token_pattern=r'(?u)\\b\\w+\\b' # Include apostrophes in words\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Ensure text data is properly formatted as strings\n",
    "    X_train_text = X_train.astype(str).fillna('')  # Handle NaN values\n",
    "    X_valid_text = X_valid.astype(str).fillna('')\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    \n",
    "    # Check if vocabulary was created\n",
    "    if len(tfidf.vocabulary_) == 0:\n",
    "        # Fallback to simpler parameters if no features were generated\n",
    "        print(\"Warning: No features generated - trying fallback parameters\")\n",
    "        tfidf = TfidfVectorizer(\n",
    "            ngram_range=(1, 1),\n",
    "            stop_words=None,\n",
    "            min_df=1,\n",
    "            max_df=1.0,\n",
    "            lowercase=True\n",
    "        )\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_valid_tfidf = tfidf.transform(X_valid_text)\n",
    "    \n",
    "    print(f\"Successfully created {len(tfidf.vocabulary_)} features\")\n",
    "    print(f\"Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in TF-IDF vectorization: {str(e)}\")\n",
    "    # Provide troubleshooting suggestions\n",
    "    print(\"Troubleshooting tips:\")\n",
    "    print(\"1. Check if X_train contains valid text data\")\n",
    "    print(\"2. Verify there are no empty strings or problematic characters\")\n",
    "    print(\"3. Try reducing min_df or increasing max_df parameters\")\n",
    "    print(\"4. Examine sample text:\", X_train.head(3).values)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad79e19-db78-426e-96f0-92363323d94b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_reg_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Logistic Regression Evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y_pred_valid = \u001b[43mlog_reg_cv\u001b[49m.predict(X_valid_tfidf)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLogistic Regression Validation Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_valid, y_pred_valid))\n",
      "\u001b[31mNameError\u001b[39m: name 'log_reg_cv' is not defined"
     ]
    }
   ],
   "source": [
    "#Logistic Regression Evaluation\n",
    "y_pred_valid = log_reg_cv.predict(X_valid_tfidf)\n",
    "\n",
    "print(\"Logistic Regression Validation Results:\")\n",
    "print(classification_report(y_valid, y_pred_valid))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_valid, labels=log_reg_cv.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=log_reg_cv.classes_, yticklabels=log_reg_cv.classes_)\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.savefig(\"../results/confusion_logistic.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad25a08-4507-4a83-90c6-1a2b0a7fd8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Evaluation\n",
    "y_pred_valid_rf = rf_cv.predict(X_valid_tfidf)\n",
    "\n",
    "print(\"Random Forest Validation Results:\")\n",
    "print(classification_report(y_valid, y_pred_valid_rf))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_valid_rf, labels=rf_cv.classes_)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=rf_cv.classes_, yticklabels=rf_cv.classes_)\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.savefig(\"../results/confusion_rf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a68707-52b7-461a-be12-6958c8597b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Evaluation (\n",
    "print(\"===== Final Evaluation on Test Set =====\")\n",
    "\n",
    "print(\"\\n[Logistic Regression]\")\n",
    "y_pred_test = log_reg_cv.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\n[Random Forest]\")\n",
    "y_pred_test_rf = rf_cv.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_test_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd3fed-2e7d-4f8c-a055-af53eee95475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Models & Vectorize\n",
    "# Save vectorizer and best models\n",
    "pickle.dump(tfidf, open(\"../models/tfidf_vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(log_reg_cv.best_estimator_, open(\"../models/tfidf_logistic.pkl\", \"wb\"))\n",
    "pickle.dump(rf_cv.best_estimator_, open(\"../models/tfidf_rf.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… Models and vectorizer saved to /models/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
