{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338e32d7-2bb2-4a7b-9b0f-f5b3e0ce4006",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (1197512271.py, line 50)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.df['credibility_score'] = (\u001b[39m\n                                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "# Advanced Visualizations and Data Profiling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class AdvancedEDAAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "        # Verify file exists\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Dataset not found at: {data_path}\")\n",
    "            \n",
    "        self.df = pd.read_csv(data_path, sep='\\t', header=0)\n",
    "        self.setup_data()\n",
    "    \n",
    "    def setup_data(self):\n",
    "        \"\"\"Prepare data for analysis\"\"\"\n",
    "        # Define column names based on LIAR dataset structure\n",
    "        columns = ['id', 'label', 'statement', 'subject', 'speaker', 'speaker_job', \n",
    "                  'state_info', 'party_affiliation', 'barely_true_counts', \n",
    "                  'false_counts', 'half_true_counts', 'mostly_true_counts', \n",
    "                  'pants_fire_counts', 'context']\n",
    "        \n",
    "        if len(self.df.columns) == len(columns):\n",
    "            self.df.columns = columns\n",
    "        else:\n",
    "            warnings.warn(f\"Dataset has {len(self.df.columns)} columns, expected {len(columns)}. Using original column names.\")\n",
    "        \n",
    "        # Create derived features\n",
    "        self.df['text_length'] = self.df['statement'].str.len()\n",
    "        self.df['word_count'] = self.df['statement'].str.split().str.len()\n",
    "        \n",
    "        # Calculate credibility scores\n",
    "        credibility_cols = ['barely_true_counts', 'false_counts', 'half_true_counts',\n",
    "                          'mostly_true_counts', 'pants_fire_counts']\n",
    "        self.df['total_statements'] = self.df[credibility_cols].sum(axis=1)\n",
    "        self.df['credibility_score'] = (\n",
    "            (self.df['mostly_true_counts'] * 1.0 + \n",
    "             self.df['half_true_counts'] * 0.5 + \n",
    "             self.df['barely_true_counts'] * 0.25) / \n",
    "            (self.df['total_statements'] + 1e-5)  # Avoid division by zero\n",
    "    \n",
    "    def create_interactive_dashboard(self):\n",
    "        \"\"\"Create comprehensive interactive dashboard\"\"\"\n",
    "        # Create subplot figure\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            subplot_titles=('Label Distribution', 'Text Length by Label',\n",
    "                          'Credibility Score Distribution', 'Party vs Label Analysis',\n",
    "                          'Subject Category Analysis', 'Credibility vs Text Length'),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "                   [{\"type\": \"histogram\"}, {\"type\": \"heatmap\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Label Distribution\n",
    "        label_counts = self.df['label'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=label_counts.index, y=label_counts.values, \n",
    "                   name='Label Count', showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Text Length by Label - Box Plot\n",
    "        for label in self.df['label'].unique():\n",
    "            data = self.df[self.df['label'] == label]['text_length']\n",
    "            fig.add_trace(\n",
    "                go.Box(y=data, name=label, showlegend=False),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Credibility Score Distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=self.df['credibility_score'], name='Credibility', \n",
    "                        showlegend=False, nbinsx=30),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. Party vs Label Heatmap\n",
    "        party_label_crosstab = pd.crosstab(\n",
    "            self.df['party_affiliation'], self.df['label'], normalize='index'\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=party_label_crosstab.values,\n",
    "                      x=party_label_crosstab.columns,\n",
    "                      y=party_label_crosstab.index,\n",
    "                      colorscale='Blues', showscale=True,\n",
    "                      colorbar=dict(title='Proportion')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # 5. Subject Category Analysis\n",
    "        subject_counts = self.df['subject'].value_counts().head(10)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=subject_counts.values, y=subject_counts.index,\n",
    "                   orientation='h', name='Subject Count', showlegend=False),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # 6. Credibility vs Text Length\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=self.df['text_length'], y=self.df['credibility_score'],\n",
    "                      mode='markers', name='Correlation', showlegend=False,\n",
    "                      marker=dict(opacity=0.6, size=5)),\n",
    "            row=3, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=1200, width=1400,\n",
    "            title_text=\"Fake News Dataset - Comprehensive Analysis Dashboard\",\n",
    "            title_x=0.5\n",
    "        )\n",
    "        \n",
    "        # Save and show\n",
    "        os.makedirs(\"results/figures\", exist_ok=True)\n",
    "        fig.write_html(\"results/figures/interactive_dashboard.html\")\n",
    "        fig.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def create_word_clouds(self):\n",
    "        \"\"\"Generate word clouds for different truth labels\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        labels = self.df['label'].unique()[:6]\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            # Get statements for this label\n",
    "            statements = self.df[self.df['label'] == label]['statement']\n",
    "            text = ' '.join(statements.dropna().astype(str))\n",
    "            \n",
    "            # Create word cloud\n",
    "            wordcloud = WordCloud(\n",
    "                width=800, height=400, \n",
    "                background_color='white',\n",
    "                max_words=100,\n",
    "                colormap='viridis'\n",
    "            ).generate(text)\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[i].set_title(f'Word Cloud - {label.upper()}', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        os.makedirs(\"results/figures\", exist_ok=True)\n",
    "        plt.savefig('results/figures/word_clouds_by_label.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def statistical_significance_analysis(self):\n",
    "        \"\"\"Perform statistical tests\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Chi-square test for party affiliation and label\n",
    "        contingency_table = pd.crosstab(\n",
    "            self.df['party_affiliation'], self.df['label']\n",
    "        )\n",
    "        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        results['party_label_association'] = {\n",
    "            'chi2': chi2,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "        \n",
    "        # ANOVA for text length across labels\n",
    "        groups = [group['text_length'].values for name, group in \n",
    "                 self.df.groupby('label')]\n",
    "        f_stat, p_value = stats.f_oneway(*groups)\n",
    "        results['text_length_anova'] = {\n",
    "            'f_statistic': f_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "        \n",
    "        # Correlation analysis\n",
    "        numeric_cols = ['text_length', 'word_count', 'credibility_score',\n",
    "                       'total_statements']\n",
    "        correlation_matrix = self.df[numeric_cols].corr()\n",
    "        \n",
    "        # Create correlation heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',\n",
    "                   center=0, square=True, fmt=\".2f\")\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(\"results/figures\", exist_ok=True)\n",
    "        plt.savefig('results/figures/correlation_matrix.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        results['correlations'] = correlation_matrix\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_speaker_analysis(self):\n",
    "        \"\"\"Analyze speaker patterns\"\"\"\n",
    "        # Top speakers by statement count\n",
    "        speaker_counts = self.df['speaker'].value_counts().head(15)\n",
    "        \n",
    "        # Speaker credibility analysis\n",
    "        speaker_credibility = self.df.groupby('speaker').agg({\n",
    "            'credibility_score': 'mean',\n",
    "            'total_statements': 'first',\n",
    "            'label': 'count'\n",
    "        }).sort_values('label', ascending=False).head(15)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Top speakers\n",
    "        speaker_counts.plot(kind='barh', ax=ax1, color='skyblue')\n",
    "        ax1.set_title('Top 15 Speakers by Statement Count')\n",
    "        ax1.set_xlabel('Number of Statements')\n",
    "        \n",
    "        # Speaker credibility\n",
    "        speaker_credibility.plot(x='total_statements', y='credibility_score',\n",
    "                               kind='scatter', ax=ax2, s=100, alpha=0.7)\n",
    "        ax2.set_title('Speaker Credibility vs Total Statements')\n",
    "        ax2.set_xlabel('Total Statements Made')\n",
    "        ax2.set_ylabel('Average Credibility Score')\n",
    "        \n",
    "        # Annotate top points\n",
    "        for i, row in speaker_credibility.iterrows():\n",
    "            ax2.annotate(i, (row['total_statements'], row['credibility_score']),\n",
    "                         xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        os.makedirs(\"results/figures\", exist_ok=True)\n",
    "        plt.savefig('results/figures/speaker_analysis.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        return speaker_credibility\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate final EDA report\"\"\"\n",
    "        report = {\n",
    "            'dataset_overview': {\n",
    "                'total_samples': len(self.df),\n",
    "                'features': list(self.df.columns),\n",
    "                'label_distribution': self.df['label'].value_counts().to_dict(),\n",
    "                'missing_values': self.df.isnull().sum().to_dict()\n",
    "            },\n",
    "            'text_statistics': {\n",
    "                'avg_text_length': self.df['text_length'].mean(),\n",
    "                'avg_word_count': self.df['word_count'].mean(),\n",
    "                'text_length_by_label': self.df.groupby('label')['text_length'].mean().to_dict()\n",
    "            },\n",
    "            'metadata_insights': {\n",
    "                'unique_speakers': self.df['speaker'].nunique(),\n",
    "                'unique_subjects': self.df['subject'].nunique(),\n",
    "                'party_distribution': self.df['party_affiliation'].value_counts().to_dict(),\n",
    "                'avg_credibility_by_party': self.df.groupby('party_affiliation')['credibility_score'].mean().to_dict()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        os.makedirs(\"results/reports\", exist_ok=True)\n",
    "        with open('results/reports/eda_comprehensive_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create results directories\n",
    "    os.makedirs(\"results/figures\", exist_ok=True)\n",
    "    os.makedirs(\"results/reports\", exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Use training set by default\n",
    "        DATA_PATH = 'data/raw/train.tsv'\n",
    "        \n",
    "        print(f\"Starting EDA analysis with dataset: {DATA_PATH}\")\n",
    "        analyzer = AdvancedEDAAnalyzer(DATA_PATH)\n",
    "        \n",
    "        # Generate all analyses\n",
    "        print(\"Creating interactive dashboard...\")\n",
    "        dashboard = analyzer.create_interactive_dashboard()\n",
    "        \n",
    "        print(\"Generating word clouds...\")\n",
    "        analyzer.create_word_clouds()\n",
    "        \n",
    "        print(\"Performing statistical analysis...\")\n",
    "        statistical_results = analyzer.statistical_significance_analysis()\n",
    "        \n",
    "        print(\"Analyzing speaker patterns...\")\n",
    "        speaker_analysis = analyzer.create_speaker_analysis()\n",
    "        \n",
    "        print(\"Generating comprehensive report...\")\n",
    "        comprehensive_report = analyzer.generate_comprehensive_report()\n",
    "        \n",
    "        print(\"\\nAdvanced EDA Analysis Complete!\")\n",
    "        print(\"Generated files:\")\n",
    "        print(\"- results/figures/interactive_dashboard.html\")\n",
    "        print(\"- results/figures/word_clouds_by_label.png\")\n",
    "        print(\"- results/figures/correlation_matrix.png\")\n",
    "        print(\"- results/figures/speaker_analysis.png\")\n",
    "        print(\"- results/reports/eda_comprehensive_report.json\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please verify the dataset path exists\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df498ca-8029-4cd0-aa75-aac0b866fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Speaker Credibility Analysis\n",
    "def speaker_credibility_analysis():\n",
    "    \"\"\"Comprehensive speaker credibility analysis\"\"\"\n",
    "    # Calculate credibility metrics\n",
    "    credibility_cols = ['barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "                       'mostly_true_counts', 'pants_fire_counts']\n",
    "    \n",
    "    full_df['total_statements'] = full_df[credibility_cols].sum(axis=1)\n",
    "    full_df['true_ratio'] = (full_df['mostly_true_counts'] + full_df['half_true_counts']) / (full_df['total_statements'] + 1)\n",
    "    full_df['false_ratio'] = (full_df['false_counts'] + full_df['pants_fire_counts']) / (full_df['total_statements'] + 1)\n",
    "    \n",
    "    # Top speakers by statement count\n",
    "    top_speakers = full_df.groupby('speaker').agg({\n",
    "        'total_statements': 'first',\n",
    "        'true_ratio': 'first',\n",
    "        'false_ratio': 'first',\n",
    "        'statement': 'count'\n",
    "    }).sort_values('statement', ascending=False).head(15)\n",
    "    \n",
    "    # Visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Top 15 Most Active Speakers', 'Credibility Score Distribution',\n",
    "                       'True vs False Ratio by Speaker Type', 'Speaker Activity by Party'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Top speakers\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_speakers.index[:10], y=top_speakers['statement'][:10],\n",
    "               name='Statement Count'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Credibility distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=full_df['true_ratio'], nbinsx=30, name='True Ratio Distribution'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Party analysis\n",
    "    party_stats = full_df.groupby('party_affiliation').agg({\n",
    "        'true_ratio': 'mean',\n",
    "        'false_ratio': 'mean',\n",
    "        'statement': 'count'\n",
    "    }).sort_values('statement', ascending=False).head(8)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=party_stats.index, y=party_stats['true_ratio'], \n",
    "               name='Avg True Ratio'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=party_stats.index, y=party_stats['statement'], \n",
    "               name='Total Statements'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True)\n",
    "    fig.show()\n",
    "    fig.write_html('results/figures/speaker_credibility_analysis.html')\n",
    "    \n",
    "    return top_speakers, party_stats\n",
    "\n",
    "top_speakers, party_stats = speaker_credibility_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3926b-f971-4f92-a540-9382af810b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Political Bias Analysis\n",
    "def political_bias_analysis():\n",
    "    \"\"\"Analyze political bias patterns\"\"\"\n",
    "    # Create party-label crosstab\n",
    "    party_label_crosstab = pd.crosstab(full_df['party_affiliation'], full_df['label'], normalize='index')\n",
    "    \n",
    "    # Filter out parties with less than 50 statements\n",
    "    party_counts = full_df['party_affiliation'].value_counts()\n",
    "    major_parties = party_counts[party_counts >= 50].index\n",
    "    filtered_crosstab = party_label_crosstab.loc[major_parties]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(filtered_crosstab, annot=True, fmt='.3f', cmap='RdYlBu_r', \n",
    "                center=0.16, cbar_kws={'label': 'Proportion'})\n",
    "    plt.title('Truth Label Distribution by Political Party (Normalized)', fontsize=14)\n",
    "    plt.xlabel('Truth Label')\n",
    "    plt.ylabel('Political Party')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/figures/political_bias_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Subject analysis by party\n",
    "    subject_party = pd.crosstab(full_df['subject'], full_df['party_affiliation'])\n",
    "    top_subjects = full_df['subject'].value_counts().head(10).index\n",
    "    subject_party_filtered = subject_party.loc[top_subjects, major_parties]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(subject_party_filtered, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Statement Count by Subject and Political Party', fontsize=14)\n",
    "    plt.xlabel('Political Party')\n",
    "    plt.ylabel('Subject')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/figures/subject_party_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return filtered_crosstab, subject_party_filtered\n",
    "\n",
    "bias_analysis, subject_analysis = political_bias_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367832c-1dae-47eb-968c-ee3324599d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Comprehensive Data Profiling\n",
    "def create_data_profile():\n",
    "    \"\"\"Create comprehensive data profiling report\"\"\"\n",
    "    \n",
    "    profile_data = {}\n",
    "    \n",
    "    # Basic dataset info\n",
    "    profile_data['dataset_info'] = {\n",
    "        'total_samples': len(full_df),\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'validation_samples': len(valid_df),\n",
    "        'features': len(full_df.columns),\n",
    "        'memory_usage_mb': full_df.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_analysis = full_df.isnull().sum()\n",
    "    profile_data['missing_values'] = missing_analysis[missing_analysis > 0].to_dict()\n",
    "    \n",
    "    # Label distribution\n",
    "    profile_data['label_distribution'] = full_df['label'].value_counts().to_dict()\n",
    "    \n",
    "    # Text statistics\n",
    "    profile_data['text_statistics'] = {\n",
    "        'avg_text_length': full_df['text_length'].mean(),\n",
    "        'avg_word_count': full_df['word_count'].mean(),\n",
    "        'avg_sentence_count': full_df['sentence_count'].mean(),\n",
    "        'max_text_length': full_df['text_length'].max(),\n",
    "        'min_text_length': full_df['text_length'].min(),\n",
    "    }\n",
    "    \n",
    "    # Categorical variable stats\n",
    "    categorical_vars = ['subject', 'speaker', 'party_affiliation', 'state_info']\n",
    "    profile_data['categorical_stats'] = {}\n",
    "    \n",
    "    for var in categorical_vars:\n",
    "        if var in full_df.columns:\n",
    "            profile_data['categorical_stats'][var] = {\n",
    "                'unique_values': full_df[var].nunique(),\n",
    "                'most_frequent': full_df[var].mode()[0] if len(full_df[var].mode()) > 0 else 'N/A',\n",
    "                'frequency_of_most_frequent': full_df[var].value_counts().iloc[0]\n",
    "            }\n",
    "    \n",
    "    # Save profile as JSON\n",
    "    import json\n",
    "    with open('results/reports/data_profile.json', 'w') as f:\n",
    "        json.dump(profile_data, f, indent=2, default=str)\n",
    "    \n",
    "    return profile_data\n",
    "\n",
    "data_profile = create_data_profile()\n",
    "print(\"Data profile created and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5119506-ab54-467e-8b9e-686f94eb3019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     48\u001b[39m         json.dump(profile_data, f, indent=\u001b[32m2\u001b[39m, default=\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m profile_data\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m data_profile = \u001b[43mcreate_data_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData profile created and saved!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcreate_data_profile\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m profile_data = {}\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Basic dataset info\u001b[39;00m\n\u001b[32m      8\u001b[39m profile_data[\u001b[33m'\u001b[39m\u001b[33mdataset_info\u001b[39m\u001b[33m'\u001b[39m] = {\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_samples\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mfull_df\u001b[49m),\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_samples\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(train_df),\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtest_samples\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(test_df),\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalidation_samples\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(valid_df),\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(full_df.columns),\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmemory_usage_mb\u001b[39m\u001b[33m'\u001b[39m: full_df.memory_usage(deep=\u001b[38;5;28;01mTrue\u001b[39;00m).sum() / \u001b[32m1024\u001b[39m**\u001b[32m2\u001b[39m\n\u001b[32m     15\u001b[39m }\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Missing values analysis\u001b[39;00m\n\u001b[32m     18\u001b[39m missing_analysis = full_df.isnull().sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'full_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 8: Comprehensive Data Profiling\n",
    "def create_data_profile():\n",
    "    \"\"\"Create comprehensive data profiling report\"\"\"\n",
    "    \n",
    "    profile_data = {}\n",
    "    \n",
    "    # Basic dataset info\n",
    "    profile_data['dataset_info'] = {\n",
    "        'total_samples': len(full_df),\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'validation_samples': len(valid_df),\n",
    "        'features': len(full_df.columns),\n",
    "        'memory_usage_mb': full_df.memory_usage(deep=True).sum() / 1024**2\n",
    "    }\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_analysis = full_df.isnull().sum()\n",
    "    profile_data['missing_values'] = missing_analysis[missing_analysis > 0].to_dict()\n",
    "    \n",
    "    # Label distribution\n",
    "    profile_data['label_distribution'] = full_df['label'].value_counts().to_dict()\n",
    "    \n",
    "    # Text statistics\n",
    "    profile_data['text_statistics'] = {\n",
    "        'avg_text_length': full_df['text_length'].mean(),\n",
    "        'avg_word_count': full_df['word_count'].mean(),\n",
    "        'avg_sentence_count': full_df['sentence_count'].mean(),\n",
    "        'max_text_length': full_df['text_length'].max(),\n",
    "        'min_text_length': full_df['text_length'].min(),\n",
    "    }\n",
    "    \n",
    "    # Categorical variable stats\n",
    "    categorical_vars = ['subject', 'speaker', 'party_affiliation', 'state_info']\n",
    "    profile_data['categorical_stats'] = {}\n",
    "    \n",
    "    for var in categorical_vars:\n",
    "        if var in full_df.columns:\n",
    "            profile_data['categorical_stats'][var] = {\n",
    "                'unique_values': full_df[var].nunique(),\n",
    "                'most_frequent': full_df[var].mode()[0] if len(full_df[var].mode()) > 0 else 'N/A',\n",
    "                'frequency_of_most_frequent': full_df[var].value_counts().iloc[0]\n",
    "            }\n",
    "    \n",
    "    # Save profile as JSON\n",
    "    import json\n",
    "    with open('results/reports/data_profile.json', 'w') as f:\n",
    "        json.dump(profile_data, f, indent=2, default=str)\n",
    "    \n",
    "    return profile_data\n",
    "\n",
    "data_profile = create_data_profile()\n",
    "print(\"Data profile created and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4933181-cf34-4e01-863a-3d2443c89d26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- results/reports/ (analysis summaries)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- results/processed_liar_dataset.csv (processed data)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43msave_day3_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msave_day3_results\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save all Day 3 analysis results\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create summary statistics\u001b[39;00m\n\u001b[32m      6\u001b[39m summary_stats = {\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext_metrics\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mfull_df\u001b[49m[[\u001b[33m'\u001b[39m\u001b[33mtext_length\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mword_count\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msentence_count\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      8\u001b[39m                             \u001b[33m'\u001b[39m\u001b[33mavg_word_length\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcapital_ratio\u001b[39m\u001b[33m'\u001b[39m]].describe(),\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabel_distribution\u001b[39m\u001b[33m'\u001b[39m: full_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].value_counts(),\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mparty_distribution\u001b[39m\u001b[33m'\u001b[39m: full_df[\u001b[33m'\u001b[39m\u001b[33mparty_affiliation\u001b[39m\u001b[33m'\u001b[39m].value_counts(),\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msubject_distribution\u001b[39m\u001b[33m'\u001b[39m: full_df[\u001b[33m'\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Save processed dataset\u001b[39;00m\n\u001b[32m     15\u001b[39m full_df.to_csv(\u001b[33m'\u001b[39m\u001b[33mresults/processed_liar_dataset.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'full_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save All Analysis Results\n",
    "def save_day3_results():\n",
    "    \"\"\"Save all Day 3 analysis results\"\"\"\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary_stats = {\n",
    "        'text_metrics': full_df[['text_length', 'word_count', 'sentence_count', \n",
    "                                'avg_word_length', 'capital_ratio']].describe(),\n",
    "        'label_distribution': full_df['label'].value_counts(),\n",
    "        'party_distribution': full_df['party_affiliation'].value_counts(),\n",
    "        'subject_distribution': full_df['subject'].value_counts()\n",
    "    }\n",
    "    \n",
    "    # Save processed dataset\n",
    "    full_df.to_csv('results/processed_liar_dataset.csv', index=False)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    with pd.ExcelWriter('results/reports/day3_analysis_summary.xlsx') as writer:\n",
    "        summary_stats['text_metrics'].to_excel(writer, sheet_name='Text_Metrics')\n",
    "        summary_stats['label_distribution'].to_excel(writer, sheet_name='Label_Distribution')\n",
    "        summary_stats['party_distribution'].to_excel(writer, sheet_name='Party_Distribution')\n",
    "        summary_stats['subject_distribution'].to_excel(writer, sheet_name='Subject_Distribution')\n",
    "        \n",
    "        # Add top speakers and party analysis\n",
    "        top_speakers.to_excel(writer, sheet_name='Top_Speakers')\n",
    "        party_stats.to_excel(writer, sheet_name='Party_Statistics')\n",
    "    \n",
    "    print(\"All Day 3 results saved successfully!\")\n",
    "    print(f\"Files saved in:\")\n",
    "    print(f\"- results/figures/ (visualizations)\")\n",
    "    print(f\"- results/reports/ (analysis summaries)\")\n",
    "    print(f\"- results/processed_liar_dataset.csv (processed data)\")\n",
    "\n",
    "save_day3_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c21cd-70ce-4ad7-9dc4-58b1393b680d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
