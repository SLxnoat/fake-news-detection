{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574c979e-7267-45dc-9f09-a8a271fba001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae07977a-6e9a-4c24-aaa5-2b627f442abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ===== STEP 1: Load and Explore LIAR Dataset =====\n",
    "\n",
    "def load_liar_dataset():\n",
    "    \"\"\"Load LIAR dataset with proper column names\"\"\"\n",
    "    \n",
    "columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation','barely_true_counts', 'false_counts', 'half_true_counts','mostly_true_counts', 'pants_fire_counts', 'context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb13f81-3d56-4ab6-959a-8f8b3fe384f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (10240, 14)\n",
      "Valid set: (1284, 14)\n",
      "Test set: (1267, 14)\n",
      "\n",
      "Label distribution in training set:\n",
      "label\n",
      "half-true      2114\n",
      "false          1995\n",
      "mostly-true    1962\n",
      "true           1676\n",
      "barely-true    1654\n",
      "pants-fire      839\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/raw/train.tsv', sep='\\t', header=None, names=columns)\n",
    "valid_df = pd.read_csv('../data/raw/valid.tsv', sep='\\t', header=None, names=columns)\n",
    "test_df = pd.read_csv('../data/raw/test.tsv', sep='\\t', header=None, names=columns)\n",
    "\n",
    "print(f\"Train set: {train_df.shape}\")\n",
    "print(f\"Valid set: {valid_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e8fc70-876e-46be-95de-8c18ad025b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 2: BERT Feature Extractor Class =====\n",
    "\n",
    "class BERTFeatureExtractor:\n",
    "    \"\"\"Extract BERT features from text statements\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=128):\n",
    "        print(f\"Loading BERT model: {model_name}\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        \n",
    "        # Move model to device and set to evaluation mode\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"BERT model loaded successfully on {self.device}\")\n",
    "    \n",
    "    def extract_features(self, texts, batch_size=16):\n",
    "        \"\"\"\n",
    "        Extract BERT features from list of texts\n",
    "        Returns: numpy array of shape (n_texts, 768)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        print(f\"Extracting BERT features for {len(texts)} texts...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encoded = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Use [CLS] token representation (first token)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                features.append(cls_embeddings)\n",
    "        \n",
    "        # Combine all batches\n",
    "        all_features = np.vstack(features)\n",
    "        print(f\"BERT features extracted: {all_features.shape}\")\n",
    "        \n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46b0bda-c407-4f57-bf50-27d49d1a3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(self, features, labels, texts, filename):\n",
    "        \"\"\"Save extracted features to file\"\"\"\n",
    "        data = {\n",
    "            'features': features,\n",
    "            'labels': labels,\n",
    "            'texts': texts,\n",
    "            'feature_dim': features.shape[1]\n",
    "        }\n",
    "        \n",
    "        filepath = Path('data/processed') / filename\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        \n",
    "        print(f\"Features saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba134e08-741a-498a-b273-6ebc3cd2cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model: bert-base-uncased\n",
      "BERT model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "# ===== STEP 3: Extract BERT Features =====\n",
    "\n",
    "# Initialize BERT extractor\n",
    "bert_extractor = BERTFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100b8210-00a5-4150-8df1-7c2018f25c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare texts (fill NaN with empty strings)\n",
    "train_texts = train_df['statement'].fillna('').tolist()\n",
    "valid_texts = valid_df['statement'].fillna('').tolist()\n",
    "test_texts = test_df['statement'].fillna('').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb8f890-db12-430b-9a07-6bcee1b765f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on small sample...\n"
     ]
    }
   ],
   "source": [
    "# Extract features for small sample first (for testing)\n",
    "print(\"Testing on small sample...\")\n",
    "sample_size = 100\n",
    "train_sample_texts = train_texts[:sample_size]\n",
    "train_sample_labels = train_df['label'][:sample_size].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0336bc2-38f5-4d8f-8013-acbbc4b59b3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_bert_features_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Define your data FIRST\u001b[39;00m\n\u001b[32m      2\u001b[39m features_data = {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mtrain_bert_features_sample\u001b[49m, \n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: train_sample_labels,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtexts\u001b[39m\u001b[33m'\u001b[39m: train_sample_texts\n\u001b[32m      6\u001b[39m } \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Save to file\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mbert_features_sample.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'train_bert_features_sample' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# --- FIRST: DEFINE THESE VARIABLES ---\n",
    "# Option 1: Compute BERT features (example)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "train_sample_texts = [\"First text sample\", \"Second text sample\"]\n",
    "train_sample_labels = [0, 1]  # Example labels\n",
    "\n",
    "# Compute BERT features\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\n",
    "    train_sample_texts,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "train_bert_features_sample = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# --- SECOND: CREATE THE DICTIONARY ---\n",
    "features_data = {\n",
    "    'features': train_bert_features_sample, \n",
    "    'labels': train_sample_labels,\n",
    "    'texts': train_sample_texts\n",
    "} \n",
    "\n",
    "# --- THIRD: SAVE TO FILE ---\n",
    "with open('bert_features_sample.pkl', 'wb') as f:\n",
    "    pickle.dump(features_data, f)\n",
    "\n",
    "# --- FOURTH: VERIFY LOADING ---\n",
    "with open('bert_features_sample.pkl', 'rb') as f: \n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Access components (use correct keys!)\n",
    "features = loaded_data['features']  # NOT 'train_bert_features_sample'\n",
    "labels = loaded_data['labels']      # NOT 'train_sample_labels'\n",
    "texts = loaded_data['texts']        # NOT 'train_sample_texts'\n",
    "\n",
    "print(f\"Loaded features shape: {features.shape}\")\n",
    "print(f\"Loaded labels: {labels}\")\n",
    "print(f\"First text: {texts[0]}\")\n",
    "\n",
    "# --- FIFTH: FUNCTION DEFINITION ---\n",
    "def save_features(features, labels, texts, filename):\n",
    "    \"\"\"Save features, labels, and texts to a pickle file\"\"\"\n",
    "    features_data = {\n",
    "        'features': features,\n",
    "        'labels': labels,\n",
    "        'texts': texts\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(features_data, f)\n",
    "\n",
    "# --- SIXTH: TEST LOADING ---\n",
    "try: \n",
    "    with open('bert_features_sample.pkl', 'rb') as f:\n",
    "        data = pickle.load(f) \n",
    "    print(\"Success! File loaded correctly.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb106b-655e-4631-b19e-f69c6d361b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_features_sample.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Access the components\n",
    "features = loaded_data['features']\n",
    "labels = loaded_data['labels']\n",
    "texts = loaded_data['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3c7a6-a5e0-49a8-bb9f-202cebcfaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample features\n",
    "# LOAD EXISTING DATA\n",
    "with open('bert_features_sample.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Extract components (if you need to use them)\n",
    "features = loaded_data['features']\n",
    "labels = loaded_data['labels']\n",
    "texts = loaded_data['texts']\n",
    "\n",
    "#Define your data FIRST\n",
    "train_bert_features_sample = [\"train_bert_features_sample\"] \n",
    "train_sample_labels = [\"train_sample_labels\"]        \n",
    "train_sample_texts = [\"train_sample_texts\"]          \n",
    "\n",
    "#THEN create the dictionary\n",
    "features_data = {\n",
    "    'features': train_bert_features_sample,\n",
    "    'labels': train_sample_labels,\n",
    "    'texts': train_sample_texts\n",
    "}\n",
    "\n",
    "#Load data\n",
    "with open('bert_features_sample.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "#Load the same data again\n",
    "with open('bert_features_sample.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)  # ‚ùó Redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751affe-1952-4460-9128-6d80e9530af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_features_sample.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Access the components\n",
    "features = loaded_data['features']\n",
    "labels = loaded_data['labels']\n",
    "texts = loaded_data['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209b2d7-8ce8-41c8-ac54-40361ff8a13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
